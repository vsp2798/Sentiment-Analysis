{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/pingwu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/pingwu/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/pingwu/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/pingwu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pingwu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "\n",
    "try: \n",
    "    import numpy as np\n",
    "except:\n",
    "    ! pip install numpy\n",
    "    import numpy as np\n",
    "    \n",
    "try: \n",
    "    import pandas as pd\n",
    "except:\n",
    "    ! pip install pandas\n",
    "    import pandas as pd\n",
    "    \n",
    "try: \n",
    "    import sklearn\n",
    "except:\n",
    "    ! pip install sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "try: \n",
    "    import nltk\n",
    "except:\n",
    "    ! pip install nltk\n",
    "    import nltk    \n",
    "## Download Resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import words, stopwords\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "############\n",
    "try: \n",
    "    import syllables\n",
    "except:\n",
    "    ! pip install syllables\n",
    "    import syllables\n",
    "\n",
    "import string\n",
    "\n",
    "try: \n",
    "    import scipy\n",
    "except:\n",
    "    ! pip install scipy\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#import pickle\n",
    "\n",
    "try: \n",
    "    import matplotlib\n",
    "except:\n",
    "    ! pip install matplotlib\n",
    "    \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendArticles(articleList, basePath):\n",
    "    # This function appends all the support articles into one. \n",
    "    contents = ''\n",
    "    for articleNumber in articleList:\n",
    "        f = open(basePath+str(articleNumber)+\".txt\", \"r\", encoding=\"utf8\")\n",
    "        contents = f.read()+\";\"+contents\n",
    "        f.close()\n",
    "    return contents\n",
    "\n",
    "def stemTweetToWordList(text, stopWords):\n",
    "    # This function lower all the words in the text, then \n",
    "    # exclude the stop words and stem the rest.\n",
    "    ps = PorterStemmer()\n",
    "    tweet =text.lower() # lower case\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')  \n",
    "    wordList = tokenizer.tokenize(tweet)\n",
    "     # remove stop words and store the stem version. \n",
    "    return [ps.stem(word) for word in wordList if word not in stopWords]\n",
    "\n",
    "def startNumber(text):\n",
    "    # This function check if the claim start with numbers.\n",
    "    if text[:1].isdigit():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def containQuestion(text):\n",
    "    # This function check if the claim has particular punctuations.\n",
    "    if '?' in text or '!' in text:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def numberSyllable(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    number_words = len(text.split())\n",
    "    total = 0\n",
    "    for word in text.split():\n",
    "        total += syllables.estimate(word)\n",
    "    return total/number_words\n",
    "\n",
    "def assignLength(row, colName):\n",
    "    # This function returns the length of the text\n",
    "    return len(row[colName])\n",
    "\n",
    "def POSTagging(text):\n",
    "    # This function assigns part of speech tag to words\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return nltk.pos_tag(tokens) \n",
    "\n",
    "def adjectives(lists):\n",
    "    # This function counts the number of adjective words.\n",
    "    count = 0\n",
    "    adjective_tag = ['JJ','JJR','JJS']\n",
    "    for word in lists:\n",
    "        if word[1] in adjective_tag:\n",
    "            count +=1\n",
    "    return count\n",
    "\n",
    "def nouns(lists):\n",
    "    # This function counts the number of noun words.\n",
    "    count = 0\n",
    "    noun_tag = ['NN','NNS','NNP']\n",
    "    for word in lists:\n",
    "        if word[1] in noun_tag:\n",
    "            count +=1\n",
    "    return count\n",
    "\n",
    "def verbs(lists):\n",
    "    # This function counts the number of verb words.\n",
    "    count = 0\n",
    "    verb_tag = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "    for word in lists:\n",
    "        if word[1] in verb_tag:\n",
    "            count +=1\n",
    "    return count\n",
    "\n",
    "def firstWord(lists):\n",
    "    # This function checks if the first word is in the below category.\n",
    "    tag = ['VB','VBD','VBG','VBN','VBP','VBZ','NN','NNS','NNP','JJ','JJR','JJS']\n",
    "    word = lists[0]\n",
    "    if word in tag:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def lastWord(lists):\n",
    "    # This function checks if the last word is in the below category.\n",
    "    tag = ['VB','VBD','VBG','VBN','VBP','VBZ','NN','NNS','NNP','JJ','JJR','JJS']\n",
    "    word = lists[-1]\n",
    "    if word in tag:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def FPP(lists):\n",
    "    # This function counts the number of first person words.\n",
    "    count = 0\n",
    "    FPP_tag = ['I','ME','WE','US','MY','MINE','OUR','OURS']\n",
    "    for word in lists:\n",
    "        if word[0].upper() in FPP_tag:\n",
    "            count +=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from local files\n",
    "basePath = os.getcwd()\n",
    "# 0:false, 1:partly true, 2:true\n",
    "claim = pd.read_json(open(basePath + \"/train.json\", \"r\", encoding=\"utf8\"))\n",
    "txtPath = basePath+\"/articles/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all support articles into one for data cleaning purpose\n",
    "claim['articleTextRaw'] = claim.apply(lambda row: appendArticles(row['related_articles'], txtPath) ,axis=1)\n",
    "# process the article text including lower words, exclude stop words and stem them.\n",
    "stopWords = stopwords.words('english')\n",
    "claim['articleText'] = claim.apply(lambda row: stemTweetToWordList(row['articleTextRaw'], stopWords) ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use nltk sentiment analyzer to analyze the sentiment of claims, and get a score in between -1 and 1.\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "claim['claimSentiment'] = claim.apply(lambda row: sid.polarity_scores(row['claim'])['compound'] ,axis=1)\n",
    "claim['SentimentAdjust'] = claim['claimSentiment']+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if claim starts with a number\n",
    "claim['start_number'] = claim.apply(lambda row: startNumber(row['claim']) ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if claim contain any ? or !\n",
    "claim['containQX'] = claim.apply(lambda row: containQuestion(row['claim']) ,axis=1)\n",
    "#number of words in the claim\n",
    "claim['titleWords'] = claim.apply(lambda row: len(row['claim'].split()) ,axis=1)\n",
    "#number of related articles to the claim\n",
    "claim['numberArticle'] = claim.apply(lambda row: len(row['related_articles']) ,axis=1)\n",
    "#number of average syllables in each word in the claim\n",
    "claim['claimSyllable'] = claim.apply(lambda row: numberSyllable(row['claim']) ,axis=1)\n",
    "claim['articleLength'] = claim.apply(lambda row: assignLength(row, 'articleText'), axis=1)\n",
    "count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "#get number of punctuation in the claim\n",
    "claim['claimPunc'] = claim.apply(lambda row: count(row['claim'],set(string.punctuation)), axis=1)\n",
    "#get number of punctuation in each related articles\n",
    "claim['articlePunc'] = claim.apply(lambda row: count(row['articleText'],set(string.punctuation))/row['numberArticle'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of words and their POS for the claim\n",
    "claim['claimPOS'] = claim.apply(lambda row: POSTagging(row['claim']), axis=1)\n",
    "#check number of adjectives, nouns, and verbs in the claim\n",
    "claim['claimAdj'] = claim.apply(lambda row: adjectives(row['claimPOS']), axis=1)\n",
    "claim['claimNoun'] = claim.apply(lambda row: nouns(row['claimPOS']), axis=1)\n",
    "claim['claimVerb'] = claim.apply(lambda row: verbs(row['claimPOS']), axis=1)\n",
    "claim['claimPOSratio'] = claim.apply(lambda row: (row['claimAdj']+row['claimNoun']+row['claimVerb'])/len(row['claimPOS']), axis=1)\n",
    "claim['claimFirst'] = claim.apply(lambda row: firstWord(row['claimPOS']), axis=1)\n",
    "claim['claimLast'] = claim.apply(lambda row: lastWord(row['claimPOS']), axis=1)\n",
    "claim['claimFPP'] = claim.apply(lambda row: FPP(row['claimPOS']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    See full source and example: \n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    #plt.yticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes, size = 20, rotation='vertical')\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylim([-0.5, 2.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the list of words into string by space to extract features.\n",
    "claim['articleText'] = claim.apply(lambda row: ' '.join(row['articleText']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the intermediate information in order to avoid repeating the process above every time. \n",
    "# file = open('important', 'wb')\n",
    "# pickle.dump(claim, file)\n",
    "# file.close()\n",
    "# file = open('important', 'rb')\n",
    "# claim = pickle.load(file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(claim, claim['label'], test_size=0.4, random_state=10)\n",
    "X_train_text, X_test_text= X_train['articleText'], X_test['articleText']\n",
    "\n",
    "# Initialize the `tfidf_vectorizer` \n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', \\\n",
    "                                   ngram_range=(1, 1)) \n",
    "# Fit and transform the training data \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train_text) \n",
    "# Transform the test set \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "# Add the manually selected features in the data\n",
    "featureToBeAdded = ['SentimentAdjust','numberArticle','articleLength','start_number',\\\n",
    "                    'containQX','titleWords','claimSyllable','claimPunc','articlePunc',\\\n",
    "                    'claimAdj', 'claimNoun', 'claimVerb', 'claimPOSratio', 'claimFirst',\\\n",
    "                    'claimLast', 'claimFPP',\n",
    "                   ]\n",
    "combResults = tfidf_train\n",
    "for featureName in featureToBeAdded:\n",
    "    colToBeAdded = coo_matrix(X_train[featureName]).transpose()\n",
    "    combResults = hstack([combResults, colToBeAdded])\n",
    "combResultsTest = tfidf_test\n",
    "for featureName in featureToBeAdded:\n",
    "    colToBeAdded = coo_matrix(X_test[featureName]).transpose()\n",
    "    combResultsTest = hstack([combResultsTest, colToBeAdded])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the performance of potential models and select one to further tuning the hyperparameters to improve the accuracy. Potential candidates include logistic regression, naive bayes, support vector machine and k nearest neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model is logistic\n",
      "accuracy:   0.622\n",
      "the model is multi naive bayes\n",
      "accuracy:   0.524\n",
      "the model is complement naive bayes\n",
      "accuracy:   0.532\n"
     ]
    }
   ],
   "source": [
    "clf1 = LogisticRegression(random_state=0, solver='newton-cg', multi_class='multinomial', max_iter=10000, n_jobs = -1)\n",
    "clf2 = MultinomialNB() \n",
    "clf3 = ComplementNB()\n",
    "modelList = [(clf1,'logistic'), (clf2,'multi naive bayes'), (clf3,'complement naive bayes')]\n",
    "for model in modelList:\n",
    "    print('the model is %s' %model[1])\n",
    "    clf = model[0]\n",
    "    clf.fit(combResults, y_train)\n",
    "    pred = clf.predict(combResultsTest)\n",
    "    score = accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model is svc\n",
      "accuracy:   0.473\n",
      "the model is kNN\n",
      "accuracy:   0.470\n"
     ]
    }
   ],
   "source": [
    "clf4 =  SVC()\n",
    "clf5 =  KNeighborsClassifier(n_jobs=-1)\n",
    "modelList = [(clf4,'svc'), (clf5,'kNN')]\n",
    "for model in modelList:\n",
    "    print('the model is %s' %model[1])\n",
    "    clf = model[0]\n",
    "    clf.fit(combResults, y_train)\n",
    "    pred = clf.predict(combResultsTest)\n",
    "    score = accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous model performance test, it shows logistic regression does a lot better than the rest, so it is chosen to futher tune its parameters. First set of parameters to tune are regularization strength and solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C is 0.100000\n",
      "solver is newton-cg\n",
      "accuracy:   0.615\n",
      "C is 0.100000\n",
      "solver is sag\n",
      "accuracy:   0.484\n",
      "C is 0.100000\n",
      "solver is saga\n",
      "accuracy:   0.484\n",
      "C is 0.100000\n",
      "solver is lbfgs\n",
      "accuracy:   0.591\n",
      "C is 0.575000\n",
      "solver is newton-cg\n",
      "accuracy:   0.616\n",
      "C is 0.575000\n",
      "solver is sag\n",
      "accuracy:   0.484\n",
      "C is 0.575000\n",
      "solver is saga\n",
      "accuracy:   0.484\n",
      "C is 0.575000\n",
      "solver is lbfgs\n",
      "accuracy:   0.526\n",
      "C is 1.050000\n",
      "solver is newton-cg\n",
      "accuracy:   0.614\n",
      "C is 1.050000\n",
      "solver is sag\n",
      "accuracy:   0.484\n",
      "C is 1.050000\n",
      "solver is saga\n",
      "accuracy:   0.484\n",
      "C is 1.050000\n",
      "solver is lbfgs\n",
      "accuracy:   0.527\n",
      "C is 1.525000\n",
      "solver is newton-cg\n",
      "accuracy:   0.610\n",
      "C is 1.525000\n",
      "solver is sag\n",
      "accuracy:   0.484\n",
      "C is 1.525000\n",
      "solver is saga\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(claim, claim['label'], test_size=0.4, random_state=10)\n",
    "X_train_text, X_test_text= X_train['articleText'], X_test['articleText']\n",
    "\n",
    "# Initialize the `tfidf_vectorizer` \n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', \\\n",
    "                                   ngram_range=(1, 1), max_features=1000) \n",
    "# Fit and transform the training data \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train_text) \n",
    "# Transform the test set \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "featureToBeAdded = ['SentimentAdjust','numberArticle','articleLength','start_number',\\\n",
    "                    'containQX','titleWords','claimSyllable','claimPunc','articlePunc',\\\n",
    "                    'claimAdj', 'claimNoun', 'claimVerb', 'claimPOSratio', 'claimFirst',\\\n",
    "                    'claimLast', 'claimFPP',\n",
    "                   ]\n",
    "combResults = tfidf_train\n",
    "for featureName in featureToBeAdded:\n",
    "    colToBeAdded = coo_matrix(X_train[featureName]).transpose()\n",
    "    combResults = hstack([combResults, colToBeAdded])\n",
    "\n",
    "combResultsTest = tfidf_test\n",
    "for featureName in featureToBeAdded:\n",
    "    colToBeAdded = coo_matrix(X_test[featureName]).transpose()\n",
    "    combResultsTest = hstack([combResultsTest, colToBeAdded])\n",
    "    \n",
    "for C in np.linspace(0.1, 2, 5):\n",
    "    for solver in ('newton-cg', 'sag', 'saga', 'lbfgs'):\n",
    "        print('C is %f' %C)\n",
    "        print('solver is %s' %solver)\n",
    "        clf = LogisticRegression(random_state=0, solver=solver, multi_class='multinomial', \\\n",
    "                                 n_jobs = -1, max_iter=10000, C=C)\n",
    "\n",
    "        clf.fit(combResults, y_train)\n",
    "        pred = clf.predict(combResultsTest)\n",
    "        score = accuracy_score(y_test, pred)\n",
    "        print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search results show that regularization strentgh C=0.57 and solver=newton-cg achieves the best accuracy 0.616."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the best hyperparameters searched above, max_features and max_df used to extract word features in TfidfVectorizer are to be tuned. Also, single word and bigram are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features is 500.000000\n",
      "max_df is 0.500000\n",
      "accuracy:   0.608\n",
      "max_features is 500.000000\n",
      "max_df is 0.700000\n",
      "accuracy:   0.608\n",
      "max_features is 500.000000\n",
      "max_df is 0.900000\n",
      "accuracy:   0.610\n",
      "max_features is 1100.000000\n",
      "max_df is 0.500000\n",
      "accuracy:   0.613\n",
      "max_features is 1100.000000\n",
      "max_df is 0.700000\n",
      "accuracy:   0.618\n",
      "max_features is 1100.000000\n",
      "max_df is 0.900000\n",
      "accuracy:   0.618\n",
      "max_features is 1700.000000\n",
      "max_df is 0.500000\n",
      "accuracy:   0.610\n",
      "max_features is 1700.000000\n",
      "max_df is 0.700000\n"
     ]
    }
   ],
   "source": [
    "for max_features in range(500, 1800, 600):\n",
    "    for max_df in np.linspace(0.5, 0.9, 3):\n",
    "        print('max_features is %f' %max_features)\n",
    "        print('max_df is %f' %max_df)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(claim, claim['label'], test_size=0.4, random_state=10)\n",
    "        X_train_text, X_test_text= X_train['articleText'], X_test['articleText']\n",
    "\n",
    "        # Initialize the `tfidf_vectorizer` \n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english', \\\n",
    "                                           ngram_range=(1, 2), max_df=max_df, max_features=max_features) \n",
    "        # Fit and transform the training data \n",
    "        tfidf_train = tfidf_vectorizer.fit_transform(X_train_text) \n",
    "        # Transform the test set \n",
    "        tfidf_test = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "\n",
    "        featureToBeAdded = ['SentimentAdjust','numberArticle','articleLength','start_number',\\\n",
    "                            'containQX','titleWords','claimSyllable','claimPunc','articlePunc',\\\n",
    "                            'claimAdj', 'claimNoun', 'claimVerb', 'claimPOSratio', 'claimFirst',\\\n",
    "                            'claimLast', 'claimFPP',\n",
    "                            #'claimSentiment', 'claimPOS'\n",
    "                           ]\n",
    "\n",
    "        combResults = tfidf_train\n",
    "        for featureName in featureToBeAdded:\n",
    "            colToBeAdded = coo_matrix(X_train[featureName]).transpose()\n",
    "            combResults = hstack([combResults, colToBeAdded])\n",
    "\n",
    "        combResultsTest = tfidf_test\n",
    "        for featureName in featureToBeAdded:\n",
    "            colToBeAdded = coo_matrix(X_test[featureName]).transpose()\n",
    "            combResultsTest = hstack([combResultsTest, colToBeAdded])\n",
    "\n",
    "\n",
    "        clf = LogisticRegression(random_state=0, solver='newton-cg', C=0.6, multi_class='multinomial', \\\n",
    "                                 n_jobs = -1, max_iter=10000)\n",
    "\n",
    "        clf.fit(combResults, y_train)\n",
    "        pred = clf.predict(combResultsTest)\n",
    "        score = accuracy_score(y_test, pred)\n",
    "        print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a continuation of the previous grid search since combining into one cell takes too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features is 2300.000000\n",
      "max_df is 0.500000\n",
      "accuracy:   0.614\n",
      "max_features is 2300.000000\n",
      "max_df is 0.700000\n",
      "accuracy:   0.614\n",
      "max_features is 2300.000000\n",
      "max_df is 0.900000\n",
      "accuracy:   0.617\n"
     ]
    }
   ],
   "source": [
    "for max_features in range(2300, 2500, 600):\n",
    "    for max_df in np.linspace(0.5, 0.9, 3):\n",
    "        print('max_features is %f' %max_features)\n",
    "        print('max_df is %f' %max_df)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(claim, claim['label'], test_size=0.4, random_state=10)\n",
    "        X_train_text, X_test_text= X_train['articleText'], X_test['articleText']\n",
    "\n",
    "        # Initialize the `tfidf_vectorizer` \n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english', \\\n",
    "                                           ngram_range=(1, 2), max_df=max_df, max_features=max_features) \n",
    "        # Fit and transform the training data \n",
    "        tfidf_train = tfidf_vectorizer.fit_transform(X_train_text) \n",
    "        # Transform the test set \n",
    "        tfidf_test = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "\n",
    "        featureToBeAdded = ['SentimentAdjust','numberArticle','articleLength','start_number',\\\n",
    "                            'containQX','titleWords','claimSyllable','claimPunc','articlePunc',\\\n",
    "                            'claimAdj', 'claimNoun', 'claimVerb', 'claimPOSratio', 'claimFirst',\\\n",
    "                            'claimLast', 'claimFPP',\n",
    "                            #'claimSentiment', 'claimPOS'\n",
    "                           ]\n",
    "\n",
    "        combResults = tfidf_train\n",
    "        for featureName in featureToBeAdded:\n",
    "            colToBeAdded = coo_matrix(X_train[featureName]).transpose()\n",
    "            combResults = hstack([combResults, colToBeAdded])\n",
    "\n",
    "        combResultsTest = tfidf_test\n",
    "        for featureName in featureToBeAdded:\n",
    "            colToBeAdded = coo_matrix(X_test[featureName]).transpose()\n",
    "            combResultsTest = hstack([combResultsTest, colToBeAdded])\n",
    "\n",
    "\n",
    "        clf = LogisticRegression(random_state=0, solver='newton-cg', C=0.6, multi_class='multinomial', \\\n",
    "                                 n_jobs = -1, max_iter=10000)\n",
    "\n",
    "        clf.fit(combResults, y_train)\n",
    "        pred = clf.predict(combResultsTest)\n",
    "        score = accuracy_score(y_test, pred)\n",
    "        print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above results, it shows max_features = 1100.000000 and max_df=0.700000 achieves the best accuracy=0.618, a merely 0.2% improvement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentimentAdjust</th>\n",
       "      <th>numberArticle</th>\n",
       "      <th>articleLength</th>\n",
       "      <th>start_number</th>\n",
       "      <th>containQX</th>\n",
       "      <th>titleWords</th>\n",
       "      <th>claimSyllable</th>\n",
       "      <th>claimPunc</th>\n",
       "      <th>articlePunc</th>\n",
       "      <th>claimAdj</th>\n",
       "      <th>claimNoun</th>\n",
       "      <th>claimVerb</th>\n",
       "      <th>claimPOSratio</th>\n",
       "      <th>claimFirst</th>\n",
       "      <th>claimLast</th>\n",
       "      <th>claimFPP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.3182</td>\n",
       "      <td>4</td>\n",
       "      <td>680</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.7977</td>\n",
       "      <td>3</td>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>1515</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>5228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.2036</td>\n",
       "      <td>6</td>\n",
       "      <td>6620</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>1.636364</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SentimentAdjust  numberArticle  articleLength  start_number  containQX  \\\n",
       "0           1.3182              4            680             0          0   \n",
       "1           0.7977              3            895             0          0   \n",
       "2           1.0000              3           1515             0          0   \n",
       "3           1.0000              3           5228             0          0   \n",
       "4           0.2036              6           6620             0          0   \n",
       "\n",
       "   titleWords  claimSyllable  claimPunc  articlePunc  claimAdj  claimNoun  \\\n",
       "0          12       1.500000          2          0.0         1          5   \n",
       "1          15       2.400000          1          0.0         0         11   \n",
       "2          15       1.600000          3          0.0         1          5   \n",
       "3          18       1.666667          1          0.0         1          7   \n",
       "4          22       1.636364          6          0.0         1          5   \n",
       "\n",
       "   claimVerb  claimPOSratio  claimFirst  claimLast  claimFPP  \n",
       "0          1       0.500000           0          0         0  \n",
       "1          2       0.812500           0          0         0  \n",
       "2          5       0.687500           0          0         0  \n",
       "3          4       0.631579           0          0         0  \n",
       "4          7       0.464286           0          0         1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claim[featureToBeAdded].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspecting the data in claim, it is found that many features have values from hundreds to thousands, so attempt is made to scale them back to the same range as the word features and see if this action could improve the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.617\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(claim, claim['label'], test_size=0.4, random_state=10)\n",
    "X_train_text, X_test_text= X_train['articleText'], X_test['articleText']\n",
    "\n",
    "###\n",
    "#preprocess data\n",
    "normalizedList =['numberArticle', 'articleLength', 'titleWords','claimPunc', 'articlePunc', 'claimNoun',\\\n",
    "                 'claimVerb']\n",
    "for column in normalizedList:\n",
    "    scaler = StandardScaler()\n",
    "    X_train[column] = scaler.fit_transform(pd.DataFrame(X_train[column])) - \\\n",
    "                                      min(scaler.fit_transform(pd.DataFrame(X_train[column])))\n",
    "    X_test[column] = scaler.transform(pd.DataFrame(X_test[column])) - \\\n",
    "                                      min(scaler.transform(pd.DataFrame(X_test[column])))\n",
    "###\n",
    "\n",
    "# Initialize the `tfidf_vectorizer` \n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', \\\n",
    "                                   ngram_range=(1, 2), max_df=0.7, max_features=1100) \n",
    "# Fit and transform the training data \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train_text) \n",
    "# Transform the test set \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "featureToBeAdded = ['SentimentAdjust','numberArticle','articleLength','start_number',\\\n",
    "                    'containQX','titleWords','claimSyllable','claimPunc','articlePunc',\\\n",
    "                    'claimAdj', 'claimNoun', 'claimVerb', 'claimPOSratio', 'claimFirst',\\\n",
    "                    'claimLast', 'claimFPP',\n",
    "                   ]\n",
    "\n",
    "combResults = tfidf_train\n",
    "for featureName in featureToBeAdded:\n",
    "    colToBeAdded = coo_matrix(X_train[featureName]).transpose()\n",
    "    combResults = hstack([combResults, colToBeAdded])\n",
    "\n",
    "combResultsTest = tfidf_test\n",
    "for featureName in featureToBeAdded:\n",
    "    colToBeAdded = coo_matrix(X_test[featureName]).transpose()\n",
    "    combResultsTest = hstack([combResultsTest, colToBeAdded])\n",
    "\n",
    "\n",
    "clf = LogisticRegression(random_state=0, solver='newton-cg', C=0.6, multi_class='multinomial', \\\n",
    "                         n_jobs = -1, max_iter=10000)\n",
    "\n",
    "clf.fit(combResults, y_train)\n",
    "pred = clf.predict(combResultsTest)\n",
    "score = accuracy_score(y_test, pred)\n",
    "print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows scales the features does not improve the accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix of a typical run results is shown. It shows the model has trouble classifying false and partial true, and it also has trouble recognizing true claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, pred, labels=[0, 1, 2])\n",
    "plot_confusion_matrix(cm, classes=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demostrate feature Importance for each of the three class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zeroClass = []\n",
    "oneClass = []\n",
    "twoClass = []\n",
    "listClass = [zeroClass, oneClass, twoClass]\n",
    "for idx in range(clf.coef_.shape[0]):\n",
    "    for position, each in enumerate(clf.coef_[idx]):\n",
    "        listClass[idx].append((each,position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class False\n",
      "data\n",
      "race\n",
      "deport\n",
      "bush\n",
      "spend\n",
      "nomin\n",
      "think\n",
      "contribut\n",
      "sponsor\n",
      "2013\n",
      "pay\n",
      "center\n",
      "fair\n",
      "pictur\n",
      "campaign\n",
      "reform\n",
      "fund\n",
      "2007\n",
      "opportun\n",
      "amend\n",
      "paid\n",
      "district\n",
      "ye\n",
      "return\n",
      "trend\n",
      "busi\n",
      "militari\n",
      "differ\n",
      "rate\n",
      "prohibit\n",
      "#############\n",
      "Class Partial True\n",
      "special\n",
      "begin\n",
      "consist\n",
      "arm\n",
      "open\n",
      "commun\n",
      "quickli\n",
      "taxpay\n",
      "la\n",
      "diseas\n",
      "advis\n",
      "east\n",
      "extend\n",
      "cancer\n",
      "adopt\n",
      "sen\n",
      "grant\n",
      "offic\n",
      "self\n",
      "sanction\n",
      "mail\n",
      "000\n",
      "13\n",
      "sens\n",
      "noth\n",
      "director\n",
      "feature number 1102 is not in the list\n",
      "feature number 1073 is not in the list\n",
      "regist\n",
      "50\n",
      "#############\n",
      "Class True\n",
      "ban\n",
      "threat\n",
      "putin\n",
      "40\n",
      "expans\n",
      "job\n",
      "florida\n",
      "individu\n",
      "labor\n",
      "applaus\n",
      "titl\n",
      "design\n",
      "white hous\n",
      "presid trump\n",
      "author\n",
      "organ\n",
      "attack\n",
      "forc\n",
      "fake\n",
      "obama\n",
      "russia\n",
      "facebook\n",
      "quot\n",
      "2019\n",
      "evid\n",
      "revenu\n",
      "claim\n",
      "news\n",
      "fact\n",
      "studi\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(listClass)):\n",
    "    if idx == 2:\n",
    "        listClass[idx] = sorted(listClass[idx],reverse=True)\n",
    "    else:\n",
    "        listClass[idx] = sorted(listClass[idx],reverse=False)\n",
    "featureNameList = tfidf_vectorizer.get_feature_names()\n",
    "print('Class False')\n",
    "for each in listClass[0][:30]:\n",
    "    print(featureNameList[each[1]])\n",
    "print('#############')\n",
    "print('Class Partial True')\n",
    "for each in listClass[1][485:515]: \n",
    "    if each[1] < 1000:\n",
    "        print(featureNameList[each[1]])  \n",
    "    else:\n",
    "        print('feature number %d is not in the list' %each[1])\n",
    "print('#############') \n",
    "print('Class True')\n",
    "for each in listClass[2][-30:]:\n",
    "    print(featureNameList[each[1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
